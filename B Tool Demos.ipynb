{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7ee2f7-fda5-4b17-b301-56c6aca1df21",
   "metadata": {},
   "source": [
    "# [CPSC 325](https://github.com/GonzagaCPSC325) Data Science Project Lab\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Tool Demos\n",
    "What are our learning objectives for this lesson?\n",
    "* Follow along with some cloud data science demos\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Google Cloud Platform training courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ee931-a1c7-4ffb-99ef-4eba6e51100d",
   "metadata": {},
   "source": [
    "## Warm up Task(s)\n",
    "1. Go to https://www.allmysportsteamssuck.com/ncaa-division-i-football-and-basketball-twitter-hashtags-and-handles/ and inspect/view the page source\n",
    "    * In Chrome, right click -> Inspect or View Page Source\n",
    "1. Suppose we want to extract the Men's Basketball Team Twitter handles, what are the relevant tags do we need to find in the the HTML?\n",
    "\n",
    "## Today\n",
    "* Announcements\n",
    "    * Project pitch: IoT water flow with microcontroller, microphone, and pub/sub\n",
    "    * Research proposal is due Thursday night, let's go over it\n",
    "* Go over lambda function solutions\n",
    "* Project brainstorming lab speed dating!!\n",
    "* Webscraping\n",
    "* TODO before next class: please finish the last page of the project brainstorming lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ccd91-c347-45dc-8a6c-58e601f58d54",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "We can scrape data we are interested in from web pages. While there are several libraries to help you do this, today we will use [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/):\n",
    "> Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n",
    "\n",
    "## Tasks\n",
    "1. Make a new project called WebScrapingFun\n",
    "1. Use Beautiful Soup to scrape the school names and handles from https://www.allmysportsteamssuck.com/ncaa-division-i-football-and-basketball-twitter-hashtags-and-handles/\n",
    "1. Store rankings in a Pandas DataFrame and write it to a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbacb50-0daf-4e70-ac69-a012bf553920",
   "metadata": {},
   "source": [
    "## Warm up Task(s)\n",
    "1. Open your project from last class (I called mine WebScrapingFun, it is pushed to Github)\n",
    "\n",
    "## Today\n",
    "* Announcements\n",
    "    * Research proposal is due tonight, questions?\n",
    "    * Let's go over the project log\n",
    "* Project brainstorming lab idea sharing!!\n",
    "* Finish web scraping demo\n",
    "* Start working with Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5b7b3-2a9d-4ee3-b1f4-b50e5d085681",
   "metadata": {},
   "source": [
    "## Twitter API\n",
    "We will use the [Twitter API](https://developer.twitter.com/en/docs/twitter-api) to get account information and tweets from basketball teams. For ease of use, we will take advantage of the [Tweepy](https://docs.tweepy.org/en/stable/) Python library to help us out. \n",
    "\n",
    "## Tasks\n",
    "1. Setup\n",
    "    1. Go to https://developer.twitter.com/ and sign up for the API\n",
    "        1. Make a project, an app, and generate a \"Bearer Token\" for authenticating with the app\n",
    "        1. Copy this token we will need it later\n",
    "    1. Install Tweepy with `pip install tweepy`\n",
    "    1. Add the Python .gitignore file to your project: https://github.com/github/gitignore\n",
    "1. WebScrapingFun: pick a Twitter handle you want to work with, or just use @Zag_MBB\n",
    "    1. We will use this Tweepy function to get information about this account: https://docs.tweepy.org/en/stable/client.html#tweepy.Client.get_user\n",
    "        * We are mostly interested in the ID, since it is unique to the account and doesn't change like a handle (AKA username) can\n",
    "        * But there is so much infr you can grab!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8180af37-e1ce-45a4-aa08-b4abcae8d959",
   "metadata": {},
   "source": [
    "## Warm up Task(s)\n",
    "1. Go to Canvas -> Announcements and follow the instructions to redeem a Google Cloud Platform (GCP) coupon\n",
    "1. Run WebScrapingFun/main.py and copy the user_id for the account you were grabbing info from (@ZagMBB's is 602989093)\n",
    "1. Create a new project called CloudFunctionFun\n",
    "    * In a main.py, paste the user_id into a new variable\n",
    "    * Read the docs for: https://docs.tweepy.org/en/stable/client.html#tweepy.Client.get_users_tweets\n",
    "    * We will need the Twitter API bearer token again\n",
    "        * Either copy twitter_keys.json and read in the variable OR\n",
    "        * In the terminal you are going to run main.py, add an environment variable for your Twitter API bearer token\n",
    "            * In the docker container: `export BEARER_TOKEN=\"your token here\"`\n",
    "\n",
    "## Today\n",
    "* Announcements\n",
    "    * Nice research proposals! More details would be helpful\n",
    "    * Your homework for the next few weeks is to work on your research and keep track of your progress in your log :)\n",
    "    * We are going to be working with could services these next few days. **Don't forget to shut services down!!**\n",
    "* Requesting recent tweets from an account by ID\n",
    "* Deploying this recent tweet code a GCP Cloud Function\n",
    "* Scheduling the Cloud Function to run periodically\n",
    "* (if time) Inserting the recent tweets into a big query table\n",
    "\n",
    "## Google Cloud Functions\n",
    "From https://cloud.google.com/functions:\n",
    ">**Simplified developer experience and increased developer velocity**  \n",
    "Cloud Functions has a simple and intuitive developer experience. Just write your code and let Google Cloud handle the operational infrastructure. Develop faster by writing and running small code snippets that respond to events. Streamline challenging orchestration problems by connecting Google Cloud products to one another or third party services using events.  \n",
    "**Pay only for what you use**  \n",
    "You are only billed for your function’s execution time, metered to the nearest 100 milliseconds. You pay nothing when your function is idle. Cloud Functions automatically spins up and backs down in response to events.  \n",
    "**Avoid lock-in with open technology**  \n",
    "Use open source FaaS (function as a service) framework to run functions across multiple environments and prevent lock-in. Supported environments include Cloud Functions, local development environment, on-premises, Cloud Run, and other Knative-based serverless environments.\n",
    "\n",
    "## Tasks\n",
    "1. Setup\n",
    "    * Redeem cloud coupon and make a billing account with an associated project\n",
    "    * Enable Cloud Functions\n",
    "1. CloudFunctionFun: pick a Twitter user id to get recent tweets from\n",
    "    * We will use this Tweepy function to get tweets using the user id: https://docs.tweepy.org/en/stable/client.html#tweepy.Client.get_users_tweets\n",
    "    * We will deploy this tweet fetching code as a Cloud Function\n",
    "    * Then trigger the Cloud Function with Cloud Scheduler\n",
    "        * Note: this cron expression helper is helpful: https://crontab.cronhub.io/\n",
    "    * We will insert these tweets as rows in a BigQuery table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e7da8-8b8a-4691-b3d0-117fc9385616",
   "metadata": {},
   "source": [
    "## Warm up Task(s)\n",
    "1. Open CloudFunctionFun\n",
    "1. [`get_users_tweets`](https://docs.tweepy.org/en/stable/client.html#tweepy.Client.get_users_tweets) accepts a keyword arg called `start_time`: \n",
    ">start_time (datetime.datetime | str | None) –\n",
    "YYYY-MM-DDTHH:mm:ssZ (ISO 8601/RFC 3339). The oldest or earliest UTC timestamp from which the Tweets will be provided. Only the 3200 most recent Tweets are available. Timestamp is in second granularity and is inclusive (for example, 12:00:01 includes the first second of the minute). Minimum allowable time is 2010-11-06T00:00:00Z\n",
    "1. Provide a `start_time` that is 24 hours ago from \"now\"\n",
    "\n",
    "## Today\n",
    "* Announcements\n",
    "    * Did your cloud function execute yesterday at 10:40am? :)\n",
    "    * Work on your research, keep track of progress in your log\n",
    "    * Note on citing AI generation tools: https://www.nature.com/articles/d41586-023-00191-1\n",
    "    * Question: One more demo next week on Flask app w/ GCP Cloud Run? Or are you good?\n",
    "* Finish CloudFunctionFun\n",
    "    * Inserting 24 hours of recent tweets into a big query table\n",
    "* CloudPubSubFun\n",
    "    * Publish tweet \"messages\" to a pub/sub topic\n",
    "    * Subscribe to topic \"message\" via a Cloud Function\n",
    "\n",
    "## Pub/Sub\n",
    "From https://cloud.google.com/pubsub:\n",
    ">Ingest events for streaming into BigQuery, data lakes or operational databases.  \n",
    "**Stream analytics and connectors**  \n",
    "Native Dataflow integration enables reliable, expressive, exactly-once processing and integration of event streams in Java, Python, and SQL.  \n",
    "**In-order delivery at scale**  \n",
    "Optional per-key ordering simplifies stateful application logic without sacrificing horizontal scale—no partitions required.  \n",
    "**Cost-optimized ingestion with Pub/Sub Lite**  \n",
    "Complementing Pub/Sub, Pub/Sub Lite aims to be the lowest cost option for high-volume  event ingestion. Pub/Sub Lite offers regional or zonal storage, putting you in control of capacity management.\n",
    "\n",
    "## Tasks\n",
    "1. Setup\n",
    "    * Create a new project folder called CloudPubSubFun\n",
    "    * In a main.py, read in your bearer_token from twitter_keys.json\n",
    "1. CloudPubSubFun: pick 1 to 5 Twitter accounts to stream tweets from\n",
    "    * We will subclass this Tweepy class to stream tweets using a filtered rule: https://docs.tweepy.org/en/stable/streamingclient.html\n",
    "    * We will create a pub/sub topic and publish our streaming tweets as messages to it\n",
    "    * We will create a Cloud Function that subscribes to these messages\n",
    "        * Optionally we can have this function insert the message into our tweets table in BigQuery\n",
    "    * We will move our streaming code over to the cloud via a [free tier e2-micro Compute Engine VM](https://cloud.google.com/free/docs/free-cloud-features#free-tier-usage-limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41dad1-0555-4572-a0a1-f1d1017116be",
   "metadata": {},
   "source": [
    "## Warm up Task(s)\n",
    "1. Open CloudPubSubFun\n",
    "1. Go to https://console.cloud.google.com/\n",
    "    1. Search for, open, and enable the Compute Engine service\n",
    "    1. We are going to make a free tier VM and start a command that takes a while to run\n",
    "        * `sudo apt-get install python3-pip`\n",
    "\n",
    "## Today\n",
    "* Announcements\n",
    "    * Don't forget to update your progress log\n",
    "* Finish CloudPubSubFun\n",
    "    * Publish tweet \"messages\" to a pub/sub topic\n",
    "    * Subscribe to topic \"message\" via a Cloud Function\n",
    "        * Insert \"messages\" into BigQuery\n",
    "    * Stream tweets via Compute Engine\n",
    "* Google Cloud CLI Setup\n",
    "    \n",
    "## Compute Engine\n",
    "From https://cloud.google.com/compute:  \n",
    ">Secure and customizable compute service that lets you create and run virtual machines on Google’s infrastructure.  \n",
    "**Scale out workloads (T2A, T2D)**  \n",
    "Tau VMs are the lowest cost solution for scale-out workloads on Compute Engine, with up to 42% higher price-performance compared to general-purpose VMs of any of the leading public cloud vendors. Choose from x86 or Arm-based VMs to meet your workload and business requirements.  \n",
    "**General purpose workloads (E2, N2, N2D, N1)**  \n",
    "E2, N2, N2D, and N1 are general-purpose machines offering a good balance of price and performance, and are suitable for a wide variety of common workloads including databases, development and testing environments, web applications, and mobile gaming. They support up to 224 vCPUs and 896 GB of memory.  \n",
    "**Ultra-high memory (M2, M1)**  \n",
    "Memory-optimized machines offer the highest memory configurations with up to 12 TB for a single instance. They are well suited to memory-intensive workloads such as large in-memory databases like SAP HANA, and in-memory data analytics workloads.  \n",
    "**Compute-intensive workloads (C3, C2, C2D)**  \n",
    "Compute-optimized machines provide the highest performance per core on Compute Engine and are optimized for workloads such as high performance computing (HPC), game servers, and latency-sensitive API serving.\n",
    "**Most demanding applications and workloads (A2)**  \n",
    "Accelerator-optimized machines are based on the NVIDIA Ampere A100 Tensor Core GPU. Each A100 GPU offers up to 20x the compute performance compared to the previous generation GPU. These VMs are designed for your most demanding workloads such as machine learning and high performance computing.\n",
    "    \n",
    "## Google Cloud CLI Setup\n",
    "Pretty much everything we have done in the Google Cloud Console GUI to setup services and whatnot, we can do from command line with the Google Cloud CLI. From https://cloud.google.com/cli:\n",
    ">Create and manage Google Cloud resources and services directly on the command line or via scripts using the Google Cloud CLI. With broad platform compatibility and service coverage, perform common platform tasks faster and control your cloud resources at scale.\n",
    "\n",
    "To set the CLI up in your dev environment, download and install the Google Cloud CLI: https://cloud.google.com/sdk/docs/install\n",
    "\n",
    "Note: if your dev environment is your Docker container, run these commands at the CLI:\n",
    "* `apt-get install apt-transport-https ca-certificates gnupg`\n",
    "* `echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list`\n",
    "* `curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | tee /usr/share/keyrings/cloud.google.gpg`\n",
    "* `apt-get update && apt-get install google-cloud-cli`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
